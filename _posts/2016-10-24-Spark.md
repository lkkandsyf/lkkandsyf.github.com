---
layout: post
category : Big-data
tagline: "Supporting tagline"
tags : [ spark ]
---
大数据分析平台Spark
---
<!--more-->
---


## Jupyter-spark

我们可以使用Apache提供的toree来使用。

[https://toree.incubator.apache.org/documentation/user/quick-start.html](https://toree.incubator.apache.org/documentation/user/quick-start.html)


启动了jupyter，可直接在http://localhost:4040/jobs/来监控所有的作业。

由于Spark的开源项目更新比较快，一般情况以[官网](http://spark.apache.org)为主.

---

## Spark基础

**Spark-shell**

用Spark-shell来开发程序,在Spark-shell中`已经`创建一个名为sc的SparkContext对象和sqlcontext的SQL context对象,在4个CPU核上运行Spark-shell:

```C
cd `echo $SPARK_HOME`
./bin/spark-shell --master local[4]
#如果指定jar路径
./bin/spark-shell --master local[4] --jars testcode.jar
```
其中，--master用来设置context将要连接并使用的`资源主节点`，master的值是Standalone模式的Spark集群地址，Meson或者YARN集群的URL,或者是一个local地址，使用--jars可以添加Jar的路径，使用`逗号`分割可以`添加多个包`，Spark-shell的本质是在后台调用了spark-submit脚本来启动`应用程序`.

__加载text文件__

Spark创建sc之后，就可以加载本地文件创建RDD,我们可以加载Spark自带的本地README.md文件进行测试，返回一个MapPartitionsRDD文件
```scala
scala>val textFile = sc.textFile("file:///$SPARK_HOME/README.md")
```
需要说明的是，加载HDFS文件和本地文件都是使用textFile,区别是添加前缀(hdfs://和file://)进行标识，从本地读取文件直接返回MapPartitionsRDD,而从HDFS读取的文件先转换成HadoopRDD,然后`隐式转换成MapPartitionsRDD`。上面说的MapPartitionsRDD和HadoopRDD都是直接基于Spark的`弹性分布式数据集(RDD)`.

对于RDD，可以执行Transformation返回新的RDD，也可以执行Action得到返回结果

```scala
// 获取RDD文件textFile的第一项
scala>textFile.first()
scala>textFile.count()
```
接下来通过Transformation操作，使用filter命令返回一个新的RDD，即抽取文件全部条目的一个子集，返回一个新的FilteredRDD

```scala
//抽取含有"spark"的子集,可以链接多个Transformation和Action进行操作
scala>textFile.filter(line => line.split("Spark")).count()
```
通过RDD进行组合，来实现找出文本中`每行最多的单词数`，`词频统计`等。

1.找出文本中每行最多单词数，基于RDD的Transformation和Action可以用作更复杂的运算

```scala
scala>textFile.map(line => line.split(" ").size).reduce((a,b) => if (a > b) a else b)
```
split("")进行分词，并统计分词后的单词数，创建一个基于单词数的新的RDD，然后针对该RDD执行Reduce操作使用(a,b)=\> if (a,b) a else b

2.词频统计

从MapReduce开始，词频统计已经成为大数据处理最流行的入门程序，类似MapReduce，Spark也能很容易实现MapReduce

```scala
//	结合flatMap,map和reduceBykey来计算文件中每个单词的词频
scala>val wordCount = textFile.flatMap(line => line.split(" ")).map(word => (word,1)).reduceByKey((a,b) => a + b)
// 使用collect聚合单词统计结果
scala>wordCount.collect()
```
结合flatMap,map和reduceByKey来计算文件中每个单词的词频，并返回(string,int)类型的键值对ShuffleRDD(由于reduceByKey执行时需要进行Shuffle操作，返回的是一个Shuffle形式的RDD，ShuffleRDD)，最后使用collect聚合单词统计计算结果

__note__:如果想让Scala的函数文本更加的简洁，可以使用`占位符"_"`，占位符可以看做表达式里需要被`填入`的`空白`，这个`空白`在每次函数被调用时，由函数的`参数填入`.

当每个参数在函数文本中最多出现一次的 情况下，可以使用"_+_"扩展成两个参数的函数文本；在`多个下划线指代多个参数`，而不是`单个参数的重复使用`。第一个下划线代表第一个参数，第二个下划线代表第二个参数，依次类推。

下面通过占位符来对词频进行统计，进行优化
```scala
scala>val wordCount = textFile.flatMap(_.split(" ")).map(_,1).reduceBeKey(_+_)
```
默认的Spark是`不进行排序的`，如果以排序的方式输出，需要进行key和value互换，然后采取sortByKey的方式，可以指定降序(false),升序(true),这样就完成的数据统计的排序
```scala
scala>val wordCount = textFile.flatMap(_.split(" ")).map(_,1).reduceBeKey(_+_).map(x => (x._2,x._1)).sortByKey(false).map(x => (x._2,x._1))
```
上面的代码通过第一个x=>(x._2,x._1)实现key和value的互换，然后通过sortByKey(false)实现降序排列，通过第二个x => (x._2,x._1)再次互换，最后完成排序.

RDD 缓存 Spark也支持将数据集存放到一个集群的内存缓冲中，当数据被饭费访问时，如果在查询一个小而热的数据集，或运行一个像PageRank的迭代算法，是非常有用的，缓冲textFile变量

```scala
scala>textFile.cache()
scala>textFile.count()
```
通过cache缓存数据可以用于`非常大的数据集，支持跨越几十或几百个节点`。

**spark-submit**

写好app之后

```C
/SPARK_HOME/bin/spark-submit --master "local[4]" xxx.app(xxx.py or xx.jar)		#zsh
```

Usage:spark-submit [options] <app jar| python file> [app options]

./spark-submit -h来查看参数列表

```C
参数名称								含义

--master MATER_URL		可以是spark://host:port,mesos://host:port,yarn,yarn-cluster,yarn-client,local
--deploy-mode DEPLOY_MODE	Driver程序运行的地方，client或者cluster
--class CLASS_NAME		主类名称，含包名
--name NAME			Application名称
--conf				spark的配置文件
--jar JARS			Driver依赖的第三方jar包，多个jar包通过逗号隔开
--py-files PY_FILES		用逗号隔开的放置在Python应用程序PYTHONPATH上的.zip.egg,.py文件列表
--files FILES			用逗号隔开放置在每个executor工作目录的文件列表
--properties-file FILE		设置应用程序属性的文件路径，默认是conf/spark-defaults.conf
--dirver-memory MEM		Driver程序使用内存的大小
--driver-java-options
--driver-library-path		Driver程序的库路径
--dirver-class-path		Driver程序的类路径
--executor-memory MEM		executor内存大小，默认是1G
--dirver-cores NUM		Driver程序使用的CPU个数，仅限Spark Alone模式
--supervise			失败后是否重启Driver，仅限Spark Alone模式
--total-executor-cores NUM	executor使用的总核数，仅限Spark Alone，Spark on Mesos模式
--executor-cores NUM		每个executor使用的内核数，默认是1，仅限Spark on Yarn模式
--queue QUEUE_NAME		提交应用程序给哪个YARN的队列，默认是default队列，仅限Spark on Yarn模式
--num-executors NUM		启动executor数量，默认是2个，仅限Spark on Yarn模式
--archive ARCHIVE		仅限Spark on Yarn模式
```
note:常见的部署策略是从同一物理位置，即同一个网关的服务器上提交应用程序，在这种设置中，采用Client模式比较合适，在Client模式中，Driver直接在用户的spark-submit进程中启动，应用程序的输入和输出连接到控制台(console)，因此，这个模式对于涉及REPL的应用程序尤为合适。

目前Standalone部署模式，Mesos集群模式，和Python编写的应用程序`不支持Cluster`模式


__spark-submit提交方式__

master如下：
 + local	以单线程本地运行Spark(完全无并行)
 + local[K]	在本地以K个Worker线程运行Spark(这个数字设为你机器CPU核数的数目比较理想)
 + local[\*]	以与你机器上逻辑核数目相同的Worker线程运行Spark
 + spark://HOST:PORT	直接连接到给定的Spark独立模式集群上的Master，该端口必须设置好，可供使用的，一般是默认是7070
 + yarn-client	以Client模式连接到YARN集群，该集群的位置可以在HADOOP\_CONF\_DIR变量中找到
 + yarn-cluster	以Cluster模式连接到ARN集群，该集群的位置可以在HADOOP\_CONF\_DIR变量中找到

具体代码

```scala
#Local模式在4个CPU核上提交应用程序
$YOUR_SPAKR_HMOE/bin/spark-submit --classs "simpleApp" --master local[4] xxxx.jar argv
#Standalone模式提交应用程序
$YOUR_SPAKR_HMOE/bin/spark-submit --classs "simpleApp" --master spark://*.*.*.*:7077 --executor 2G --total-executor-core 10 xxxx.jar argv
#YARN模式提交应用程序
$YOUR_SPAKR_HMOE/bin/spark-submit --classs "simpleApp" --master yarn-cluster --executor 2G --total-executor-core 10 xxxx.jar argv
```


[Spark: zsh no matches found local](http://zpjiang.me/2015/10/17/zsh-no-match-found-local-spark/)


**pyspark**

[Apache Python API docs](http://spark.apache.org/docs/latest/api/python/)



**基本概念**

Spark应用程序由两部分组成，Driver,Executor.

 + Application :Spark的应用程序，包含一个Driver program和若干个Executor.
 + SparkContext:Spark应用程序的入口，`负责调度各个运算资源，协调各个Worker Node上的Executor`
 + Driver program:运行Application的main()函数并且创建SparkContext,通常SparkContext代表driver program
 + Executor:是Application运行在Work Node上的一个`进程`,该进程负责运行Task,并且负责将数据存在内存上或者磁盘上；每个Application都会申请各自的Executor来处理。
 + Cluster Manager:在集群上获取资源的外部服务(example:Standalone,Mesos,Yarn)
 + work Node:集群中任务可以运行Application代码的节点,运行一个或者多个Executor进程
 + Job：可以被拆分成Task并行计算的工作单元，一般由Spark Action触发的一次执行作业
 + Stage：每个Job会被拆分很多组任务(Task),每组任务被称为Stage,也称TaskSet
 + Task:运行在Executor上的工作单元
 + RDD：Resilient Distributed
 + Dataset的简称，弹性分布式数据集，是Spark最核心的模块和类，通过Scala集合转化，读取数据集合生成或者由其他RDD经过算子操作得到

Spark编程模型的最主要的抽象，第一个抽象是RDD(Resilient Distributed Dataset，弹性分布式数据集)，它是一个特殊的`集合`，支持多种来源，有容错机制，可以被缓存，支持并行操作；Spark的第二个抽象是两种共享变量，即支持并行计算的`广播变量`和`累加器`.

__RDD__

Spark的最基本抽象，是分布式内存的抽象使用，实现了以操作本地集合的方式来操作分布式数据集的抽象体现，RDD是Spark最核心的东西，它表示已被分区，不可变的并能够被并行操作的数据集合，不同的数据集格式对应不同的RDD实现，RDD必须是可`序列化的，RDD可以cache到内存中`

特点

 + 创建：只能同坐转换(transformation,如map/filter/groupBy/join等，区别与action)从两种数据源中创建RDD：稳定存储中的数据，其他RDD
 + 只读：状态`不可改变，不能改变`
 + 分区：支持使RDD中的元素根据那个key来分区(partitioning)，保存到多个结点上，还原时只会重新计算丢失分区的数据，而不会影响整个系统
 + 路径：在RDD中叫世族或血统(lineage),即RDD有充足的信息关于它是如何从其他RDD产生而来的
 + 持久化：支持将会被重用的RDD缓存(如in-memory或溢出到磁盘)
 + 延迟计算：Spark也会延迟计算RDD，使其能够转换管道化(pipeline transformation)
 + 操作:丰富的动作(action),count/reduce/collect/save等

执行了多少次的transformation操作，RDD都不会真正执行运算(记录lineage)，只有当action操作执行时，运算才会触发。

优点

 + RDD只能从持久存储或通过Transformation操作产生，相比于分布式共享内存(DSM)可以更高效实现`容错`，对于丢失部分数据分区只需要根据它的lineage就可以重新计算出来，而不需要做特定的Checkpoint
 + RDD的不变性，可以实现类Hadoop MapReduce的推断式执行
 + RDD的数据分区特性，可通过数据的本地特性来提供性能，这与Hadoop　MapReduce是一样的
 + RDD都是可序列化的，在内存不足的可`自动降级为磁盘存储`，把RDD存储于磁盘上，这时性能可能会有大的下降但不会很差
 + 批量操作：任何能够根据数据本地(data locality)被分片，从而提高性能

每个RDD都包含`五部分信息`,即数据分区的集合，能根据本地性快速访问到数据的`偏好位置，依赖关系，计算方法，是否是哈希/范围分区的元数据`

note:分区，最佳位置，依赖，函数，分区策略,其中，分区，一系列的依赖关系和函数是三个基本特征。最佳位置和分区策略是可选特征。

常用的RDD特征说明

```C
RDD名称		|	分区			|	依赖		|	函数			|	最佳位置		|	分区策略	|
HadoopRDD	|	HDFS Block		|	无		|	读取每一个Block块		|	HDFS Block位置	|	无	|
MappedRDD	|	与父RDD一样		|	与父RDD一对一	|	读取分区中每一行数据	|	本地位置		|	无	|
FilteredRDD	|	与父RDD一样		|	与父RDD一对一	|	计算父RDD每个分区并过滤	|	无(与父RDD一致)	|	无	|
JoinedRDD	|	每个Reduce任务一个分区	|	依赖所有父RDD	|	读取Shuffle数据并计算	|	无	|	HashPartitioner	|
```


RDD将依赖划分成两种两种类型

 + 窄依赖(narrow dependencies)：是指父RDD的每个分区都只被子RDD的一个分区所使用，如map就是一种窄依赖.窄依赖的RDD可以通过相同的键进行`联合分区`，整个操作都可以在一个集群节点上进行，以流水线(pipeline)的方式计算所有父分区，不会造成网络之间的数据混合。
 + 宽依赖(wide dependencies)：是指父RDD的分区被多个子RDD的分区所依赖，如join则会导致宽依赖.宽依赖RDD会涉及数据混合，宽依赖，需要首先计算好所有的父分区数据，然后再节点之间Shuffle.

窄依赖能够有效的进行失效节点的恢复，重新计算丢失RDD分区，不同节点之间可以并行计算；而对于以宽依赖的关系的血统(lineage)图，单个节点失效可能导致这个RDD的所有祖先丢失部分分区，因而需要整体重新计算。

note:Shuffle执行是固化操作，以及采取Persist缓存策略，可以在固化点，或者缓存点重新计算。

执行时，调度程序会检查依赖性的类型，将窄依赖的RDD划到一组处理当中，即Stage。宽依赖在一个执行中会跨越连续的Stage，同时需要显式的指定多个子RDD的分区。

这样划分有两个好处:

首先，窄依赖支持在一个结点上管道化执行。例如基于一对一的关系，可以在filter之后执行map

其次，窄依赖支持更高效的故障还原。因为对于窄依赖，只有丢失的父RDD的分区需要重新计算

对于宽依赖，一个结点的故障可能导致来之所有的父RDD的分区丢失，因此就需要完全重新执行。因此对于宽依赖，Spark会在持有各自父分区的结点上，将中间数据持久化来简化故障还原。

子RDD的每个分区依赖于常数个父分区，与`数据规模`无关，输入输出是一对一的算子。当子RDD的每个分区依赖`单个父分区`时,`分区结构不会发生变化`，如Map，flatMp；当子RDD依赖多个`多个父分区时`，`分区结构发生变化`。

RDD操作

转换(transformation)现有的RDD通过转换生成一个新的RDD,转换是延时执行(lazy)的

动作(action)在RDD上运行计算之后，返回的结果给驱动程序或写入文件系统，触发job

transformation

```scala
map(func)	//对调用map的RDD数据集中的每个element都使用func,然后返回一个新的RDD,这个返回的数据集是分布式数据集
filter(func) //对调用filter的RDD数据集中的每个元素都使用func，然后返回一个包含使func为true的元素构成的的RDD
flatMap()	//和map差不多，但是flatMap生的是多个结果
mapPartitions(func)	//和map很像，但map是每个element,而mapPartitions是每个partition
smaple(withReplacement,fraction.seed)	//抽样
union(otherDataset)	//返回一个新的数据集，包含源数据集和给定的数据集的元素的集合
disince([numTasks]	//返回一个新的数据集，这个数据和含有的是源数据集中的distinct的element
groupBeKey(func,[numTasks])// 返回(K,Seq[V]),也就是hadoop中的reduce函数的接受的key-valuelist
reduceByKey(func,[numTasks])//就是一个给定的reducefunc再作用在groupBeKey产生的(K,Seq[V]),比如，求和求平均数
sortByKey([ascending],[numTasks])//按照Key进行排序，是升序还是降序，ascending是布尔类型
join(otherDataset,[numTasks])//当两个KV的数据集(K,V)和(K,W)，返回是(K,(V,W))的数据集的numTasks是为并发的任务数
cogroup(otherDataset,[numTasks])// 当有两个KV的数据集(K,V)和(K,W)，返回的是(K,Seq[V],Seq[W])的数据集，numTasks为并发的任务数
cartesain(otherDataset)//笛卡尔积m*n

```

__键-值转换__

常用的键值转换
 + groupByKey([numTasks]):当在一个由键值对(K,V)组成的数据集上调用时，按照key进行分组，返回一个(K,Iterator<V>)对的数据集。`note`:如果是为了按照key聚合数据(如求和或平均值)而进行分组，使用reduceByKey或combinBykey方法会产生更好的性能。默认情况下，输出的并行程序取决于父RDD的分区数，可以通过传递一个可选的numtasks参数设置不同的并行任务数。
 + reduceByKey(func,[numsTasks]):当在一个键值对(K,V)数据集上调用时，按照key将数据分组，使用给定的func聚合values值，返回一个键值对(K,V)数据集，其中func函数的类型必须是(V,V)-\>V,类似于groupByKey,Reduce并行任务数可通过可选的第二个参数配置。
 + sortByKey([ascending],[numTasks]):返回一个以key排序(升序或降序)的(K,V)键值对组成的数据集，其中布尔型参数ascending决定升序还是降序，而numTasks为并行任务数目.
 + join(otherDataset,[numTasks]):根据key连接两个数据集，将类型为(K,V)和(K,W)的数据集合并成一个(K,(V,W))类型的数据集。外连接通过leftouterjoin和rightouterjoin获得支持。其中，numTasks为并行的任务数目。
 + cogroup(otherDataset,[numTasks]):当在两个形如(K,V)和(K,W)的键-值对数据集上调用时，返回一个(K,Iterator<V>,Iterator<W>)形式的数据集。


```scala
scala> val data = List(("a",1),("b",1),("c",1),("a",2),("b",2),("c",2))
data: List[(String, Int)] = List((a,1), (b,1), (c,1), (a,2), (b,2), (c,2))

scala> val rdd = sc.parallelize(data)
rdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[0] at parallelize at <console>:29

scala> val rbk = rdd.reduceByKey(_+_).collect
rbk: Array[(String, Int)] = Array((a,3), (b,3), (c,3))

scala> val gbk = rdd.groupByKey().collect
gbk: Array[(String, Iterable[Int])] = Array((a,CompactBuffer(1, 2)), (b,CompactBuffer(1, 2)), (c,CompactBuffer(1, 2)))

scala> val sbk = rdd.sortByKey().collect
sbk: Array[(String, Int)] = Array((a,1), (a,2), (b,1), (b,2), (c,1), (c,2))

```


actions

```scala
reduce(func)//聚集，但是传入的函数是两个参数返回一个值，这个函数必须满足交换律和结合律
collect()//一般在filter或者足够小的结果的时候，再用collect封装返回一个数组
count()	//返回的是数据集的element的个数
first()	//返回的是数据集的第一个元素
take(n)	//返回前n个element,这个是driver program返回的
takeSample(withReplacement,num,seed)//抽样返回一个数据集中的num个元素，随机种子seed
saveAsTextFile(path)//把数据集写到一个textfile中，或者hdfs，或者支持的文件系统中，spark把每条记录都转换为一行记录，然后写到file中。
saveAsSequenceFile(path)//只能用在key-value对上，然后生成SequenceFile写到本地或者hadoop文件系统
saveAsObjectFile(path)//把数据写到一个java序列化的文件中，用SparkContext.objectFile()装载
countByKey()// 返回的是key对应的个数的一个map,作用于一个RDD
foreach(func)//对dataset中的每个元素都使用func
```


缓存的操作

使用persist和cache方法将任意的RDD缓存到内存，磁盘文件系统中

缓存是容错的可以通过构建它的transformation自动重构

被缓存的RDD被使用的时，存取速度会被大大加速

persisit可以指定一个StorageLevel

控制操作主要包括故障恢复，数据持久性，以及移除数据。其中缓存操作Cache/Pesist是惰性的，在进行执行操作时，才会执行，而Unpesist是即时的，会`立即释放内存`。checkpoint会直接将RDD持久化到磁盘或HDFS等路径，不同于Cache/Persist的是，被checkpoint的RDD`不会因为作业的结束而被消除，会一直存在`，并可以被后续的作业直接读取并加载。

在一个典型的分布式系统中，容错机制主要是采取检查点(checkpoint)机制和数据备份机制。故障恢复是有主动检查，以及不同机器之间的数据复制实现的。由于进行故障恢复需要跨集群网络来复制大量数据，这无疑是相当昂贵的。因此在Spark中则采取了不同数的方法进行故障恢复。作为大型的分布式集群，Spark针对工作负载会做出两种假设。
 + 处理时间是有限的
 + 保持数据持久性是外部数据源的职责，主要是让处理过程中的数据保持稳定。
跨宽依赖的再执行能够涉及多个父RDD，从而引发全部的再执行。为了规避这一点，Spark会保持Map阶段中间数据的输出的持久，在机器发生故障的情况下，再执行只需回溯到Mapper持续输出的相应分区，来获取中间数据.

Spark还提供了数据检查点和记录日志，用于获取持久化中间RDD，这样再执行就不必追溯到`最开始的阶段`。通过比较恢复延迟和检查点开销进行权衡，Spark会自动化地选择相应的策略进行故障恢复。

Spark的持久化，是指在不同转换操作之间，将过程数据缓存在内存中，实现快速重用，或者故障快速恢复。持久化主要分为两类，主动持久化和自动持久化。自动持久化，主要目标是RDD重用，从而实现快速处理，是Spark构建迭代算法的关键。可用persist()方法来标记一个持久化的RDD，一旦被一个执行(action)触发计算，它将会被保留在计算节点的内存中并重用.此外，每一个RDD都可以用不同的`保存级别`进行保存，从而允许持久化数据集在硬盘或内存作为序列化的Java对象，甚至`跨节点复制`。持久化的等级选择，是通过将给StorageLevel对象传递给persis()方法进行确定的，cache()方法调用persist()的默认级别MEMORY\_ONLY.

持久化等级
 + MEMORY\_ONLY:将RDD作为反序列化的对象存储于JVM中，如果内存不足，一些分区将不会被缓存，在需要的时会被重新计算
 + MEMORY\_AND\_DISK:将RDD作为反序列化的对象存储在JVM中，如果内存不足，超出的分区将被保存的磁盘上，并且需要时被读取，重点:不会立即输出数据到磁盘
 + MEMORY\_ONLY\_SER:将RDD作为序列化的Java对象进行存储(每一个分区占用一个字节的数组)，通常来说，这比对象反序列化的空间利用率更高，尤其当使用fast serializer时，但在读取会比较占用CPU
 + MEMORY\_AND\_DISK\_SER:与MEMORY\_ONLY\_SER相似，但超出内存的分区将存储在硬盘上，而不是在每次需要时重新计算。
 + DISK\_ONLY:只将RDD分区存储在硬盘上
 + MEMORY\_ONLY\_2,MEMORY\_AND\_DISK\_2:与上述存储级别一样，但是将每一个分区都复制到两个两个集群节点上
 + OFF\_HEAP:将RDD以序列化格式存储在Tachyon中。

相对于MEMORY\_ONLY\_SER,OFF\_HEAP减小了垃圾回收的开销，同事也允许Executor变得更小且可共享内存设备，Executor的崩溃不会导致内存中的缓冲丢失，在这种模式下，Tachyon中的内存是不可丢弃的。

自动持久化，是指不需要用户调研persist(),Spark自动的保存一些Shuffle操作(reduceByKey)的中间结果。这样的做是为了避免在Shuffle过程中一个节点崩溃时重新计算所有的输入。

持久化时，一旦设置了就`不能改变`，想要改变就要先`去持久化`。推荐用于在重用RDD结果时调用Persist(),这样会使持久化变得`可控`

Persist()持久化RDD，修改了RDD的meta info中的StorageLevel。而检查点在持久化的同时切断了Lineage，修改了RDD的meta info中的Lineage.两者均可以返回经过修改的RDD对象自身，而非新的RDD对象，也均属于Lazy操作。

__选择存储等级__

Spark的不同存储等级，旨在`满足内存的使用和CPU效率权衡上的不同需求`，建议通过以下步骤进行选择。

 + 如果你的RDD可以很好的与默认的存储级别(MEMORY\_ONLY)契合，那么久不需要做任何的修改，这已经是CPU使用效率最高的选项，它是RDD的操作尽可能的快。
 + 如果不能与MEMORY\_ONLY很好的契合，建议使用MEMORY\_ONLY\_SER并选择一个快速序列化的库，使对象在较高空间利用率的情况下，依然可以较好的被访问。
 + 尽可能不要将数据存储到硬盘上，除非计算数据集的函数，计算量特别大，或者它们过滤的大量的数据。否则，重新计算第一个分区的速度与从硬盘中读取的效率差不多。
 + 如果想拥有`更快故障恢复能力`,可使用复制存储级别(用Spark来响应Web应用的请求)。所有的存储级别都有通过重新计算丢失数据恢复错误的容错机制，但是复制存储级别可以让你在RDD上持续的运行任务，而不需要等待丢失的分区被重新计算。
 + 如果想要定义自己的存储级别(如复制因子为3而不是2)，可以使用StorageLevel单例对象的apply()方法

RDD可以随意在RAM中进行缓存，因此它提供了更快的数据方法，目前，缓存的粒度为RDD级别，只能缓存全部的RDD。Spark自动监视每个节点上使用的缓存，在集群中没有足够的内存，Spark会根据缓存情况确定一个LRU(Least Recently Used,最近最少使用算法)的数据分区进行删除。

如果想手动删除RDD，而不想等待它从缓存中消失，可以使用RDD的unpersist()方法移除数据，unpersist()方法是立即生效的。

__创建SparkContext步骤__

 + 导入Spark的类和隐式转换
 + 构建Spark应用程序的应用信息对象SparkConf
 + 利用SparkConf对象来初始化SparkContext
 + 创建RDD,并执行相应的Transformation和action并得到最后的结果

Spark提供两种模式的共享变量:广播变量和累加器,Spark的第二个抽象便是可以在并行计算中使用的共享变量.
 + 广播变量:可以在内存的所有节点中被访问，用于缓存变量(只读)
 + 累加器:只能用来做加法的变量，如计数和求和。

广播变量(Broadcast Variables)

广播变量缓存到各个节点的内存中，而不是每个Task

广播变量被创建后，能在集群中运行的任何函数调用

广播变量是只读的，不能在被广播后修改

对于大数据集的广播，Spark尝试使用高效的广播算法来降低通信成本

```scala
val broadcastVar = sc.broadcast(Array(1,2,3))
broadcastVar.value
res0 : Array[Int] = Array(1,2,3)
```
在广播变量被创建后，可以在集群运行的任何函数中代替v值被调用，由于v值在第一次调用后缓存到任务节点，重复调用时不需要被再次传递到这些节点上。另外，对象v不能再广播后修改，这样可以保证所有的节点收到相同的广播值

累加器

累加器只支持加法操作，可以高效地并行，用于实现计数器和变量求和

Spark原生支持数值类型和标准可变集合的计数器，但用户可以添加新的类型

只有驱动程序才能获取累加器的值

Spark原生支持Int和Double类型的累加器，程序员可以自己添加新的支持类型。

code:

```scala
scala>val accum = sc.accumulator(0)
accum:org.apache.spark.Accumulator[Int] = 0
scala>sc.parallelize(Array(1,2,3,4)).foreach(x => accum += x)
....
scala>accum.value
res1:Int = 10
```

**作业执行解析**

__基本组件__

Spark基本的组件，包括负责集群运行的Master和Worker,负责作业运行的Client和Driver，以及负责集群资源管理器(YARN)和执行单元(Executor)等，并在组件的基础上，对于基于RDD数据集的`静态视图`和基于Partition的动态视图进行了说明。为接下来的作业执行流程做了铺垫。

从架构上来说，每一个Spark Application都由控制集群的`主控节点Master`，负责集群资源管理的Cluster Manager，执行具体任务的Worker节点和执行单元Executor，负责作业的提交的Client端和负责作业控制的Driver进程组成.

SparkClient负责任务的提交，Driver进程进程通过运行用户定义的main的函数，在集群上执行各种`并发操作和计算`。其中SparkContext是应用程序与集群交互的唯一通道，主要包括：获取数据，交互操作，分析和构建DAG图，通过Scheduler调度任务，Block跟踪，Shuffle跟踪。

用户通过Client提交一个程序给Driver之后，Driver会将所有RDD的依赖关联在一起绘制一张DAG图；当运行任务时，调度Scheduler会配合组件Block Tracker和Shuffle Tracker进行工作；通过ClusterManager进行资源统一调配；具体任务在Worker节点进行，由Task线程池负责具体任务执行，线程池通过多个Task运行任务。由BlockManager和StraggleingTask快速恢复。

在图论中，如果一个有向图`无法从任一顶点`经过若干条边回到该点，则这个图是一个有向无环图(Directed Acyclic Graph,DAG),是描述一项工程进行过程中的有效工具。

在Spark中，DAG图绘制完毕，`不会被立即执行，而是仅仅对数据进行标记`。

__作业执行流程__

提交作业有两种方式，分别是Driver运行在集群中，Driver运行在客户端，接下来分别介绍两种方式的作业运行原理。无论基于哪种运行方式，基础概念是一致的。

 + Stage,一个Spark作业一般包含一到多个Stage
 + Task,一个Stage包含一到多个Task，通过多个Task实现并行运行的功能
 + DAGScheduler,实现将Spark作业分解成一到多个Stage，每个Stage根据RDD的Partition个数决定Task的个数，然后生成相应的TaskSet放到TaskScheduler中。

`基于Standalone模式的Spark架构`

在Standalone模式下，有两种运行方式：以Driver运行在Worker上和以Driver运行在客户端，这两种方式可以通过参数--delpoy-mode进行配置，默认是Client模式(即Driver运行在客户端)。集群启动Master与Worker进程，Master负责接收客户端提交的作业，管理Worker，并提交Web展示集群和作业信息.

整个框架下，各种进行的角色：

 + Master:主控节点，负责接收client提交的作业，管理Worker，并命令Worker启动Driver和Executor。
 + Worker：Slaves节点上的守护进程，负责管理本节点的资源，定期向Master汇报心跳，接收Master的命令，启动Driver和Executor。
 + Client:客户端进行，负责提交作业到Master。
 + Driver：一个Spark作业运行时包括一个Driver进程，也是作业的主进程，负责DAG图的构建，Stage的划分，Task的管理及调度以及生产SchedulerBackend用于Akka通信。主要组件包括DAGScheduler,TaskScheduler以及SchedulerBackend。
 + Executor:执行作业的顶峰。每个Application一般会对应多个Worker，但是一个Application在每个Worker上只会产生一个Executor进行，每个Executor接收Driver的命令LaunchTask，一个Executor可以执行一到多个Task。

提交一个任务到集群，以Standalone为例：首先启动Master，然后启动Worker，启动Worker时要向Master注册。

Master收到应用程序提交之后，需要注册并加载Driver，然后由Driver注册应用程序，由Master去Launch具体的Executor资源，由Driver去触发Executor进程Launch具体的Task。

作业执行流程详细描述：

客户端提交作业给Master，Master让一个Worker启动Driver，即SchedulerBackend。Worker创建一个DriverRunner线程，DriverRunner启动SchedulerBackend进程。另外，Master还会让其余Worker启动Executor，即ExecutorBackend。Worker创建一个ExecutorRunner线程，ExecutorRunner会启动ExecutorBackend线程。ExecutorBackend启动后会向Driver的SchedulerBackend注册。SchedulerBackend进程中包含DAGScheduler，它会根据用户线程生成执行计划，并调度执行。对于每个Stage的Task，都会被存放到TaskScheduler中，ExecutorBackend向SchedulerBackend汇报时把TaskScheduler中的Task调度到ExecutorBackend执行。所有Stage都完成后作业结束。

总结过程：client ---> master 分支 变成一对多开始管理和调度。实时汇报情况。

程序在执行过程中，由Worker节点向Master发送心跳，随时汇报Worker的健康状况。针对几种`故障情况`，给出了相应的处理方案。

 + Worker节点出现故障。Worker在退出之前，会将该Worker上的Executor杀掉；而Worker是需要定期发送心跳给Master的，Master通过心跳机制感知该Worker节点的故障，而后将该情况汇报给Driver，并将该Worker从节点中移除；这样Driver即可知道该Worker上对应的Executor已被杀死。
 + Executor出现问题。ExecutorRunner会将情况汇报给Master，从而Master便知道Executor出现问题。但是此时运行该Executor的Worker是正常的，因此Master会发送LaunchExecutor指令给该Worker，让其`再次启动一个Executor`；而Worker收到LaunchExecutor指令后便再启动Executor。
 + Master出现故障，通过Zookeeper搭建Master的HA，一个作为Active，其他的作为Standby，一旦Active节点出现故障，能够及时切换。

Driver运行在客户端，和Driver运行在Worker节点上类似,但也有不同点。

`基于YARN模式的Spark架构`

在YARN模式下有两种运行方式：Driver运行在集群NodeManager和Driver运行在客户端。

SparkAppMaster相当于Standalone模式下的SchedulerBackend，Executor相当于Standalone模式下的ExecutorBackend，SparkAppMaster包括DAGScheduler和YARNClusterScheduler。

基于YARN的Spark作业首先由客户端生成作业信息，提交给ResourceManager，ResourceManager在某一NodeManager汇报时把AppMaster分配给NodeManager，NodeManager启动SparkAppMaster，SparkAppMaster启动后初始化作业，然后向ResourceManager申请资源，申请到相应资源后SparkAppMaster通过RPC让NodeManager启动相应的SparkExecutor，SparkExecutor向SparkAppMaster汇报并完成相应的任务。此外，SparkClient会通过AppMaster获取作业运行状态。

作业事件流和调度分析

任务提交到集群，集群将任务分配到具体的工作节点去处理。运行任务由4个参数，targetRDD,partitions,func,listens.runJob会把代码提交给DAGScheduler，DAGScheduler将代码绘制为DAG图，而根据依赖关系又将DAG图划分为不同的Stage，对应多个TaskSet，TaskSet交给TaskScheduler，TaskScheduler与资源服务器进行交互，资源管理器在根据不同的部署模式与集群进行交互，当然也可以在Local级别进行交互，TaskScheduler有自己的事件处理机制，task finish 和stage failure都是事件触发的。

作业处理调度框架

整个调度过程包括:生成RDD对象，构建DAGScheduler，任务调度，作业执行等几个部分。

1.生成RDD对象过程中，根据输入RDD进行解析，构建操作DAG图(build,operator DAG).代码中RDD进行转换(transformation)操作时惰性的(lazy)的，转换操作只会产生标记，并不会立即执行，只有遇到执行(action)操作时，执行操作调用runJob方法，从而提交至DAGScheduler，并绘制DAG图，程序才会真正执行.

2.构建DAGScheduler过程中，根据DAG划分任务阶段(split graph into stages of tasks),并将按照阶段(stage)提交任务集(TaskSet)

首先将整个DAG图划分成一个完整的Stage(也称finalStage)，然后从Stage中最后一个RDD起往前回溯。在回溯的过程中，不断判断RDD的依赖关系，如果是窄依赖(narrow dependncy)则继续回溯，如果是宽依赖(wide dependncy)则划分出一个新的Stage，从而将整个DAG图划分成多个Stage，每个Stage有一组Task组成。

如果满足:localExecutionEnabled为真；allowLocal为真；finalStage没有父State；partition数组为1这四个条件，则会在本地开启Local模式执行任务。否则，采取集群模式运行任务。只有满足该Stage没有未执行完毕的父Stage，则该Stage可以提交任务，并将任务集(Taskset)作为Stage的参数提交。

3.任务调度过程中，通过集群管理器分配资源启动具体任务(launch tasks via cluster manager),并重试失败或运行较慢任务(retry failed or straggling tasks)

DAGScheduler递交给TaskScheduler的是TaskSet，SchedulerBackend通过makeOffers申请资源，通过resourceOffer调度资源。另外TaskScheduler是通过TaskSetManager来管理这些Task的，当满足一定的条件，包括调度策略(FIFO和FAIR)以及延时调度(数据本地性)，则会将LaunchTask消息发送给ExecutorBackend。

当有任务失败或者Straggling Tasks时，在TaskScheduler层面会重新计算这些Task，只有超出Task的范围上升至Stage或者DAG层面时，才会交由DAGScheduler进行处理。任务失败之后，Stragging Tasks采取的措施是`看谁计算完`，就要谁的结果。

4.作业执行过程中，执行任务(execute task),存储并管理数据模块(store and server block)

ExecutorBackend在接受到LaunchTask消息后，会在Executor上执行LaunchTask操作。LaunchTask操作总，会生成新的TaskRunner，并以线程的方式进行执行。在Task执行时，是需要分Task和ShuffleMapTask的，两者返回也不一样。

通过BlockMananger借口对存储进行管理，包括内存的读写和磁盘读写。其中，Shuffle的中间结果时需要写入磁盘的。


`运行时环境`

Spark中的每个应用程序都维护了自己的一套运行时的环境，该运行时环境在应用程序开始时构建，在运行程序结束时销毁。相对于所有的应用程序共有一套运行时环境的方式。极大的缓解了应用程序之间的相互影响。

总体来说，一个Spark的运行时环境由四个阶段组成。

 + 构建应用程序运行时环境
 + 将应用程转换为DAG图
 + 按照依赖关系调度执行DAG图
 + 销毁应用程序的运行环境。

1.为了运行一个应用程序，Spark首先根据应用程序资源需求构建一个运行时环境。这是通过与资源管理器交互来完成的。通常而言，存在两种运行时环境构建方式：粗粒度和细粒度。
 + 粗粒度：应用程序提交到集群之后，它在正式运行任务之前，将根据应用资源需求一次性将这些资源凑齐，之后使用这些资源运行任务，整个运行过程`不再申请新资源`
 + 细粒度：应用程序提交到集群之后，动态向集群管理申请资源，只要等到资源满足一个任务的运行，便开始运行该任务，而不必等到所有的资源全部到位。目前，基于Hadoop的MapReduce就是基于细粒度运行时环境构建方式。

对于`Spark on YARN，目前仅支持粗粒度构建方式`。

2.Spark将依赖分为窄依赖和宽依赖，基于以下两点原因;

首先，从数据混洗的角度，窄依赖的RDD可以通过相同的键进行联合分区，整个操作都可以在同一个集群节点上以流水线(pipeline)形式。例如，执行了Map后，紧接着执行Filter，不会造成网络之间的数据混洗。相反，宽依赖的RDD会涉及数据混洗，需要所有的父分区都是可用的，可能还需要进行跨节点传递数据。

其次，从失败恢复的角度考虑，窄依赖恢复更有效，只需要重新计算丢失的父分区，而且可以并行的在不同的节点进行重新计算。而宽依赖涉及RDD各级的多个父分区(parent partition),可能会导致全部重新计算。

在将应用程转换成DAG的过程中，Spark的调度程序会检查依赖关系的类型，根据RDD依赖关系将应用程序划分为若干个Stage，每个Stage启动一定的数目的任务进行并行处理。

Spark采用了`贪心算法`划分阶段，如果子RDD的分区到父RDD的分区时窄依赖，就实施经典的Fusion(融合)优化，把对应的操作划分到一个Stage，如果连续的变换子序列都是窄依赖，就可以把很多个操作并到一个Stage，直到遇到一个宽依赖。这不但减少了大量的全局屏障(barrier)，而且无须物化很多中间结果的RDD，可极大的提高性能。Spark把这个乘坐流水线(pipeline)。

宽依赖在一个执行中会跨越连续的Stage，同时需要显式指定多个子RDD的分区。

DAGScheduler将Stage划分完成后，提交实际上是通过把Stage转换为TaskSet，然后通过TaskScheduler将计算任务最终提交到集群。

3.调度DAG执行图

在该阶段中，DAGScheduler将按照依赖关系调度执行每个Stage：优先选择那些不依赖任务阶段的Stage，待这些阶段执行完成后，在调度那些需要依赖的阶段已经完成Stage。依次进行，这样一直调度下去，直到所有的阶段运行完成。

在处理某个阶段时，Spark将为之启动一定数目的Task并行运行，为了提高任务的并行率。Spark借鉴了MapReduce中的优化机制。包括数据本地性和推测执行。

 + 数据本地性
 + 推测执行
 + 采取多线程执行具体的任务
 + Executor上会有一个BlockManager模块，类似于KV系统(内存和硬盘共同作为存储设备)

不论采取何种模式的部署和经过多少次转换，典型的DAG执行流程如下:

1)RDD直接从外部数据源(HDFS,本地文件)创建

2)RDD经历一系列的Transformation(Map,flatMap,Filter,groupBy,Join),每一次都会产生不同的RDD，供下一个Transformation使用

3)当触发Action(count,collect,save,take)时，将最后一个RDD进行转换，输出到外部的数据源。

上述一系列的处理过程称为一个血统(lineage)，即DAG拓扑排序的结果。在Lineage中生成的RDD都是`不可变的`，事实上，除非被缓存，每个RDD在进入下一个Transformation操作之前都只使用一次。

如果按照MapReduce，流程中任何

**Spark SQL与DataFrame**


**Spark Streaming**



核心模块---Schedule整体

Scheduler模块是Spark最核心的模块之一，充分体现了Spark与MapReduce的不同之处，体现了Spark DAG思想的精巧和设计的优雅。

Scheduler模块分为两大主要部分,DAGScheduler和TaskScheduler.

创建RDD，经过一系列Transformation,最后Action----\>Action会触发SparkContext的rubjob方法，交给DAGSchedule处理----\>DAGScheduler将DAG生成Stage-----\>将Stage交给TaskScheduler---\>本地或者集群的Executor上运行。

DAGScheduler把一个spark作业转换成Stage的DAG，根据RDD和Stage之间的关系，找出开销最小的调度方法，然后把stage以TaskSet的形式提交给TaskScheduler

SchedulableBuilder主要负责TaskSet的调度。核心接口:getSortedTaskSetQueue,该接口返回排序后的TaskSetManager队列，该接口供TaskSchedulerImpl调用。SchedulableBuilder维护的是一棵树，跟节点是rootpool,叶子节点是TaskSetManager对象。

TaskSet优先级排序有两种算法，一种是FIFO,另一种是Fair，所以SchedulableBuilder又两种实现:FIFO　SchedulableBuilder和FairSchedulableBuilder.

FIFO(默认模式)是先进先出型，根据TaskSet对应的stageID的顺序来调度。只有一个pool---rootpool和叶子节点---TaskSetManager.

Fair(模式)是公平调度型，根据所管理的pool/TaskSetManager中正在运行的任务的数量来判断优先级。所构建的调度池是两级的结构，即rootpool管理一组pool,子pool进一步管理属于该调度池的TaskSetManager.

TaskSetManager主要负责一个taskset中task的调度和跟踪

核心接口:resourceOffer,该接口根据输入的资源在taskset内部调度一个task,主要考虑因素是Locality,该接口供TaskSchedulerImpl调度。

 + 根据task的perferredLocations得到每个Task的Localitylevel
 + resourceOffe根据资源和maxLocality(最大宽松的本地化级别)调度task
 + 最终调度task的allowedLocality是该TaskSet允许的Locality(最大不超过输入的maxLocality),该TaskSet允许的Locality最初默认值是最严格本地化级别。如果lastLaunchTime(最近一次该taskset发布的task的时间)与当前时间差超时，会放宽locality的要求，选择低一优先级的locality.
 + 在allowedLocality范围内，优先调度更local的task,也就是最好在同个进程里，次好是同个node(机器)上，再次是同机架。在allowedLocality范围内，在该taskset没有找到task,那么返回None.(上一层调用会继续查询其他的taskset是否有满足指定localityLevel的task)

SchedulerBackend是trait,封装了多种backend,用于与底层资源调度系统交互(mesos/YARN),配合TaskScheduler实现具体任务所需要的资源分配。

核心接口:reviveOffers，与TaskSchedulerImpl交互完成task的Launch

SchedluerBackend值关心资源，不关心task.提交资源供TaskSchedulerImpl分配task

ReviveOffers的实现

 + 将空闲资源(freeCore,executor,host)以及workerOfferList形式组织。
 + 调用TaskSchedulerImpl的resourceOffers(),为workerOfferList空闲资源分配相应的task
 + 调用launchTasks，向executorActor发送LaunchTask消息。


```scala
val accum = sc.accumulator(0)
sc.parallelize(Array(1,2,3,4)).foreach(x => accum += x)
accum.value
```

```scala
/*SimpleApp.scala*/
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf

object SimpleApp{
	def main(args: Array[String]){
		val		logFile = "/home/user/spark/README.md"
		val		conf	= new SparkConf().setAppName("Scala Application")
		val		sc	= new SparkContext(conf)
		val		file	= sc.textFile(logFile)
		val		count	= file.flatMap(line=>line.split(" ")).map(word=>(word,1)).reduceByKey(_+_)
		count.collect()
		count.saveAsTextFile("/home/user/code/out/aa")
	}
}
```

__并行化集合__

```scala
//SparkContext的parallelize方法生成RDD
val	rdd = sc.parallelize(Array(1 to 10))
val	rdd = sc.parallelize(Array(1 to 10),5)// 指定了parition的数量
// 参数slice：启动的executro的数量来进行　切分多个slice,每个slice启动一个Task来处理。

```

## Spark MLlib与机器学习

****
****
****
****
****

## GraphX图计算框架

****
****
****
****

## SparkR

****
****
****
****
## 实战Spark

****
****
****
****
****
****


## Spark高级

**调度管理**

**存储管理**

**监控管理**

**性能调优**

## 扩展

**Spark-jobserver**

**Spark Tachyon**

****
****

## Spark command debug

在主机端启动:

```scala
./spark-submit --master "local[2]"  --class com.nsfocus.streaming.JavaSparkPi  /home/lkk/job/demo/target/demo-1.0.jar --driver-java-options -agentlib:jdwp=transport=dt_socket,address=127.0.0.1:9904,server=y,suspend=y
```

监控主机和port进行debug,端口可以改变，只要系统没有使用就可以。

> jdb 127.0.0.1:9904

## docker-scala-spark

这里收集了一些关于scala，spark编程的docker.
dockoey/jupyter-scala
[dockerhub](https://hub.docker.com/r/dockoey/jupyter-scala/)
[github](https://github.com/dockoey/jupyter-scala)

all-spark-notebook
[dockerhub](https://hub.docker.com/r/jupyter/all-spark-notebook/)
[github](https://github.com/jupyter/docker-stacks)

## spark e-book

 + [Building a Recommendation Engine with Scala](https://www.safaribooksonline.com/library/view/building-a-recommendation/9781785282584/)
 + [mastering Apache Spark2.0](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/)

## spark source code

配置Spark源码阅读环境，首先要选择的就是IDEA,支持的非常好。之前需要添加一些依赖:

 + JDK
 + Scala
 + Sbt

[download code](http://spark.apache.org/downloads.html)

对于特定的版本:

> git clone git://github.com/apache/spark.git -b branch-2.1

等待完成，之后就可以在IDEA中导入了，选择Sbt项目，选择Use auto-import,自动下载依赖包，之后就生成的IDEA项目文件，就可以浏览源代码了.

[reading spark source code in ieda](https://linbojin.github.io/2016/01/09/Reading-Spark-Souce-Code-in-IntelliJ-IDEA/)

[搭建Spark源码研读和代码调试的开发环境](https://github.com/linbojin/spark-notes/blob/master/ide-setup.md)

[Spark Intellij](http://dragonly.github.io/note/2016/05/10/Spark+Intellij-%E8%88%92%E6%9C%8D%E7%9A%84%E6%BA%90%E7%A0%81%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE.html)

总结:到github下载文件，spark-master.zip,之后解压。进入spark根目录,运行下面的命令，来编译Spark,然后直接导入到idea中:

```C
build/mvn -T 4 -DskipTests clean package		#T表示多线程
or
mvn -T 6 -Pyarn -Phadoop-2.6 -DskipTests clean package
```
`注意`:在idea中需要设置mvn,要是自己安装的，而不是idea自带的mvn.这样就可以了。测试使用:/bin/spark-shell

执行examples中的例子,修改VM的spark是local模式或其他集群模式(yarn or meson),关键的部分是调试,需要修改的是VM:

```C
-Dspark.master=local　-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5005
```
先打包:mvn package,以SparkPi为例

```C
bin/spark-submit \
--driver-java-options "-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5005" \
--class org.apache.spark.examples.SparkPi \
examples/target/spark-examples_2.10-1.6.1.jar
```
注意到第二行的参数是告诉 spark 运行的时候设置额外的 java 参数, 就是上面复制出来的参数, 改成suspend=y是告诉 spark 启动之后暂停运行, 等待 debugger 连接之后才开始运行, 方便我们加断点.

然后 CLI 会显示Listening for transport dt\_socket at address: 5005, 表示 spark 正在等待 debugger 连接.

此时只需要在 Intellij 上打开调试文件, 加上断点, 再点Run -> Debug 'Remote', 就能开始单步追踪调试了.

远程调试job,请看[mvn来debug工程](https://lkkandsyf.github.io/debug/2016/12/25/jdb)

### 过程解析

[spark-submit](http://blog.csdn.net/lovehuangjiaju/article/details/49123975)

[SparkPi](http://blog.csdn.net/ggz631047367/article/details/53816634)

## spark cluster

docker中可以通过docker search xxxx来查看远方仓库的目录,我们采用sequenceiq的镜像，[more info](https://hub.docker.com/r/sequenceiq/spark)

从hub.docker上拉取sequenceiq/spark
```c
sudo docker pull sequenceiq/spark:1.6.0

# running the image
sudo docker run -it -p 8088:8088 -p 8042:8042 -h sandbox sequenceiq/spark:1.6.0 bash
```
-p 8088:8088 这个端口是resourcemanager 或者 集群 ，-p 8042:8042 这个端口是 nodemanager端口

test

```scala
# YARN-client mode
spark-shell \
--master yarn-client \
--driver-memory 1g \
--executor-memory 1g \
--executor-cores 1
scala> sc.parallelize(1 to 1000).count()

# YARN-cluster mode
spark-submit \
--class org.apache.spark.examples.SparkPi \
--files $SPARK_HOME/conf/metrics.properties \
--master yarn-cluster \
--driver-memory 1g \
--executor-memory 1g \
--executor-cores 1 \
$SPARK_HOME/lib/spark-examples-1.6.0-hadoop2.6.0.jar

spark-submit \
--class org.apache.spark.examples.SparkPi \
--master yarn-client \
--driver-memory 1g \
--executor-memory 1g \
--executor-cores 1 \
$SPARK_HOME/lib/spark-examples-1.6.0-hadoop2.6.0.jar

```

通过webUI来查看集群的运行情况

```C
localhost:8088 #hadoop
localhost:8042 #hadoop
docker-ip:8080 #spark-ui
```
测试Spark的功能

```c
# get ip
ifconfig
# enter spark dir
cd /usr/local/spark
# backup file
cp conf/spark-env.sh.template conf/spark-env.sh
# edit file
vi conf/spark-env.sh

# add two line code
export SPARK_LOCAL_IP=sandbox_ip(ifconfig)
export SPARK_MASTER_IP=sandbox_ip(ifconfig)
# start master slaves
./sbin/start-master.sh
./sbin/start-slave.sh spark://172.17.0.109:7077
# ./sbin/start-slave.sh spark://(ifconfig):7077
# open brower to look every node
# (ip:8080)

# spark-submit
/bin/spark-submit examples/src/main/python/pi.py




# close master
./sbin/stop-master.sh

```
## spark-e-book

这里介绍一些网上比较好的电子书,网站

 + [Spark programming guide zh](https://taiwansparkusergroup.gitbooks.io/spark-programming-guide-zh-tw/content/)


[https://github.com/databricks/learning-spark](https://github.com/databricks/learning-spark)

[https://github.com/SeikaScarlet/Learning-Spark_zh](https://github.com/SeikaScarlet/Learning-Spark_zh)

[https://github.com/jayunit100/SparkStreamingApps](https://github.com/jayunit100/SparkStreamingApps)

[https://github.com/andypetrella/spark-notebook](https://github.com/andypetrella/spark-notebook)

[https://github.com/prabeesh/SparkTwitterAnalysis](https://github.com/prabeesh/SparkTwitterAnalysis)

[https://github.com/ruoyousi/spark-streaming-examples](https://github.com/ruoyousi/spark-streaming-examples)

[https://github.com/aaronp/multi-project](https://github.com/aaronp/multi-project)

[https://github.com/adnanalvee/Spark-Scala-Sbt-Template](https://github.com/adnanalvee/Spark-Scala-Sbt-Template)

[https://github.com/mrchristine/spark-template-sbt](https://github.com/mrchristine/spark-template-sbt)

[https://github.com/notyy/scalaSnippet](https://github.com/notyy/scalaSnippet)

[https://github.com/spirom/LearningSpark](https://github.com/spirom/LearningSpark)

[深入理解Spark 核心思想与源码分析](http://blog.csdn.net/beliefer/article/details/50523205)

[SparkNet](https://github.com/amplab/SparkNet)

[spark-summit](https://spark-summit.org/)

http://www.cnblogs.com/charlotte77/p/5464457.html

http://blog.csdn.net/anzhsoft2008/article/list/1

http://blog.csdn.net/qq1010885678?viewmode=contents

http://blog.csdn.net/lovehuangjiaju?viewmode=contents

http://blog.csdn.net/yirenboy

http://www.cnblogs.com/shishanyuan/p/4699644.html

http://blog.csdn.net/javastart?viewmode=contents

http://www.jianshu.com/u/2bd9b48f6ea8

http://butterluo.iteye.com/

http://blog.csdn.net/v_july_v

http://blog.csdn.net/u012185296

http://blog.csdn.net/nsrainbow/article/details/36396007

http://spaceandtim.es/code/mesos_spark_zookeeper_hdfs_docker/

http://blog.csdn.net/huwenfeng_2011/article/details/43370891

http://blog.antlypls.com/
