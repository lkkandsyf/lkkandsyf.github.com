---
layout: post
category : Big-data
tagline: "Supporting tagline"
tags : [ spark ]
---
{% include JB/setup %}

# Overview
{:.no_toc}

* dir
{:toc}

## Jupyter-spark

我们可以使用Apache提供的toree来使用。

[https://toree.incubator.apache.org/documentation/user/quick-start.html](https://toree.incubator.apache.org/documentation/user/quick-start.html)


启动了jupyter，可直接在http://localhost:4040/jobs/来监控所有的作业。

由于Spark的开源项目更新比较快，一般情况以[官网](http://spark.apache.org)为主.

---

## Spark-command

**spark-shell**

Usage:spark-submit [options] <app jar| python file> [app options]

./spark-submit -h来查看参数列表

	参数名称								含义

	--master MATER_URL		可以是spark://host:port,mesos://host:port,yarn,yarn-cluster,yarn-client,local
	--deploy-mode DEPLOY_MODE	Driver程序运行的地方，client或者cluster
	--class CLASS_NAME		主类名称，含包名
	--name NAME			Application名称
	--conf				spark的配置文件
	--jar JARS			Driver依赖的第三方jar包，多个jar包通过逗号隔开
	--py-files PY_FILES		用逗号隔开的放置在Python应用程序PYTHONPATH上的.zip.egg,.py文件列表
	--files FILES			用逗号隔开放置在每个executor工作目录的文件列表
	--properties-file FILE		设置应用程序属性的文件路径，默认是conf/spark-defaults.conf
	--dirver-memory MEM		Driver程序使用内存的大小
	--driver-java-options
	--driver-library-path		Driver程序的库路径
	--dirver-class-path		Driver程序的类路径
	--executor-memory MEM		executor内存大小，默认是1G
	--dirver-cores NUM		Driver程序使用的CPU个数，仅限Spark Alone模式
	--supervise			失败后是否重启Driver，仅限Spark Alone模式
	--total-executor-cores NUM	executor使用的总核数，仅限Spark Alone，Spark on Mesos模式
	--executor-cores NUM		每个executor使用的内核数，默认是1，仅限Spark on Yarn模式
	--queue QUEUE_NAME		提交应用程序给哪个YARN的队列，默认是default队列，仅限Spark on Yarn模式
	--num-executors NUM		启动executor数量，默认是2个，仅限Spark on Yarn模式
	--archive ARCHIVE		仅限Spark on Yarn模式

note:常见的部署策略是从同一物理位置，即同一个网关的服务器上提交应用程序，在这种设置中，采用Client模式比较合适，在Client模式中，Driver直接在用户的spark-submit进程中启动，应用程序的输入和输出连接到控制台(console)，因此，这个模式对于涉及REPL的应用程序尤为合适。

目前Standalone部署模式，Mesos集群模式，和Python编写的应用程序`不支持Cluster`模式



**pyspark**

[Apache Python API docs](http://spark.apache.org/docs/latest/api/python/)

**spark-submit**

写好app之后

	/SPARK_HOME/bin/spark-submit --master "local[4]" xxx.app(xxx.py or xx.jar)		#zsh



[Spark: zsh no matches found local](http://zpjiang.me/2015/10/17/zsh-no-match-found-local-spark/)



**基本概念**

Spark应用程序由两部分组成，Driver,Executor.

 + Application :Spark的应用程序，包含一个Driver program和若干个Executor.
 + SparkContext:Spark应用程序的入口，`负责调度各个运算资源，协调各个Worker Node上的Executor`
 + Driver program:运行Application的main()函数并且创建SparkContext,通常SparkContext代表driver program
 + Executor:是Application运行在Work Node上的一个`进程`,该进程负责运行Task,并且负责将数据存在内存上或者磁盘上；每个Application都会申请各自的Executor来处理。
 + Cluster Manager:在集群上获取资源的外部服务(example:Standalone,Mesos,Yarn)
 + work Node:集群中任务可以运行Application代码的节点,运行一个或者多个Executor进程
 + Job：可以被拆分成Task并行计算的工作单元，一般由Spark Action触发的一次执行作业
 + Stage：每个Job会被拆分很多组任务(Task),每组任务被称为Stage,也称TaskSet
 + Task:运行在Executor上的工作单元
 + RDD：Resilient Distributed
 + Dataset的简称，弹性分布式数据集，是Spark最核心的模块和类，通过Scala集合转化，读取数据集合生成或者由其他RDD经过算子操作得到

Spark编程模型的最主要的抽象，第一个抽象是RDD(Resilient Distributed Dataset，弹性分布式数据集)，它是一个特殊的`集合`，支持多种来源，有容错机制，可以被缓存，支持并行操作；Spark的第二个抽象是两种共享变量，即支持并行计算的`广播变量`和`累加器`.

__RDD__

Spark的最基本抽象，是分布式内存的抽象使用，实现了以操作本地集合的方式来操作分布式数据集的抽象体现，RDD是Spark最核心的东西，它表示已被分区，不可变的并能够被并行操作的数据集合，不同的数据集格式对应不同的RDD实现，RDD必须是可`序列化的，RDD可以cache到内存中`

特点

 + 创建：只能同坐转换(transformation,如map/filter/groupBy/join等，区别与action)从两种数据源中创建RDD：稳定存储中的数据，其他RDD
 + 只读：状态`不可改变，不能改变`
 + 分区：支持使RDD中的元素根据那个key来分区(partitioning)，保存到多个结点上，还原时只会重新计算丢失分区的数据，而不会影响整个系统
 + 路径：在RDD中叫世族或血统(lineage),即RDD有充足的信息关于它是如何从其他RDD产生而来的
 + 持久化：支持将会被重用的RDD缓存(如in-memory或溢出到磁盘)
 + 延迟计算：Spark也会延迟计算RDD，使其能够转换管道化(pipeline transformation)
 + 操作:丰富的动作(action),count/reduce/collect/save等

执行了多少次的transformation操作，RDD都不会真正执行运算(记录lineage)，只有当action操作执行时，运算才会触发。

优点

 + RDD只能从持久存储或通过Transformation操作产生，相比于分布式共享内存(DSM)可以更高效实现`容错`，对于丢失部分数据分区只需要根据它的lineage就可以重新计算出来，而不需要做特定的Checkpoint
 + RDD的不变性，可以实现类Hadoop MapReduce的推断式执行
 + RDD的数据分区特性，可通过数据的本地特性来提供性能，这与Hadoop　MapReduce是一样的
 + RDD都是可序列化的，在内存不足的可`自动降级为磁盘存储`，把RDD存储于磁盘上，这时性能可能会有大的下降但不会很差
 + 批量操作：任何能够根据数据本地(data locality)被分片，从而提高性能

每个RDD都包含`五部分信息`,即数据分区的集合，能根据本地性快速访问到数据的`偏好位置，依赖关系，计算方法，是否是哈希/范围分区的元数据`

note:分区，最佳位置，依赖，函数，分区策略,其中，分区，一系列的依赖关系和函数是三个基本特征。最佳位置和分区策略是可选特征。

常用的RDD特征说明

```
RDD名称		|	分区			|	依赖		|	函数			|	最佳位置		|	分区策略	|
HadoopRDD	|	HDFS Block		|	无		|	读取每一个Block块		|	HDFS Block位置	|	无	|
MappedRDD	|	与父RDD一样		|	与父RDD一对一	|	读取分区中每一行数据	|	本地位置		|	无	|
FilteredRDD	|	与父RDD一样		|	与父RDD一对一	|	计算父RDD每个分区并过滤	|	无(与父RDD一致)	|	无	|
JoinedRDD	|	每个Reduce任务一个分区	|	依赖所有父RDD	|	读取Shuffle数据并计算	|	无	|	HashPartitioner	|
```


RDD将依赖划分成两种两种类型

 + 窄依赖(narrow dependencies)：是指父RDD的每个分区都只被子RDD的一个分区所使用，如map就是一种窄依赖.窄依赖的RDD可以通过相同的键进行`联合分区`，整个操作都可以在一个集群节点上进行，以流水线(pipeline)的方式计算所有父分区，不会造成网络之间的数据混合。
 + 宽依赖(wide dependencies)：是指父RDD的分区被多个子RDD的分区所依赖，如join则会导致宽依赖.宽依赖RDD会涉及数据混合，宽依赖，需要首先计算好所有的父分区数据，然后再节点之间Shuffle.

窄依赖能够有效的进行失效节点的恢复，重新计算丢失RDD分区，不同节点之间可以并行计算；而对于以宽依赖的关系的血统(lineage)图，单个节点失效可能导致这个RDD的所有祖先丢失部分分区，因而需要整体重新计算。

note:Shuffle执行是固化操作，以及采取Persist缓存策略，可以在固化点，或者缓存点重新计算。

执行时，调度程序会检查依赖性的类型，将窄依赖的RDD划到一组处理当中，即Stage。宽依赖在一个执行中会跨越连续的Stage，同时需要显式的指定多个子RDD的分区。

这样划分有两个好处:

首先，窄依赖支持在一个结点上管道化执行。例如基于一对一的关系，可以在filter之后执行map

其次，窄依赖支持更高效的故障还原。因为对于窄依赖，只有丢失的父RDD的分区需要重新计算

对于宽依赖，一个结点的故障可能导致来之所有的父RDD的分区丢失，因此就需要完全重新执行。因此对于宽依赖，Spark会在持有各自父分区的结点上，将中间数据持久化来简化故障还原。

子RDD的每个分区依赖于常数个父分区，与`数据规模`无关，输入输出是一对一的算子。当子RDD的每个分区依赖`单个父分区`时,`分区结构不会发生变化`，如Map，flatMp；当子RDD依赖多个`多个父分区时`，`分区结构发生变化`。

RDD操作

转换(transformation)现有的RDD通过转换生成一个新的RDD,转换是延时执行(lazy)的

动作(action)在RDD上运行计算之后，返回的结果给驱动程序或写入文件系统，触发job

transformation

```scala
map(func)	//对调用map的RDD数据集中的每个element都使用func,然后返回一个新的RDD,这个返回的数据集是分布式数据集
filter(func) //对调用filter的RDD数据集中的每个元素都使用func，然后返回一个包含使func为true的元素构成的的RDD
flatMap()	//和map差不多，但是flatMap生的是多个结果
mapPartitions(func)	//和map很像，但map是每个element,而mapPartitions是每个partition
smaple(withReplacement,fraction.seed)	//抽样
union(otherDataset)	//返回一个新的数据集，包含源数据集和给定的数据集的元素的集合
disince([numTasks]	//返回一个新的数据集，这个数据和含有的是源数据集中的distinct的element
groupBeKey(func,[numTasks])// 返回(K,Seq[V]),也就是hadoop中的reduce函数的接受的key-valuelist
reduceByKey(func,[numTasks])//就是一个给定的reducefunc再作用在groupBeKey产生的(K,Seq[V]),比如，求和求平均数
sortByKey([ascending],[numTasks])//按照Key进行排序，是升序还是降序，ascending是布尔类型
join(otherDataset,[numTasks])//当两个KV的数据集(K,V)和(K,W)，返回是(K,(V,W))的数据集的numTasks是为并发的任务数
cogroup(otherDataset,[numTasks])// 当有两个KV的数据集(K,V)和(K,W)，返回的是(K,Seq[V],Seq[W])的数据集，numTasks为并发的任务数
cartesain(otherDataset)//笛卡尔积m*n

```
actions

```scala
reduce(func)//聚集，但是传入的函数是两个参数返回一个值，这个函数必须满足交换律和结合律
collect()//一般在filter或者足够小的结果的时候，再用collect封装返回一个数组
count()	//返回的是数据集的element的个数
first()	//返回的是数据集的第一个元素
take(n)	//返回前n个element,这个是driver program返回的
takeSample(withReplacement,num,seed)//抽样返回一个数据集中的num个元素，随机种子seed
saveAsTextFile(path)//把数据集写到一个textfile中，或者hdfs，或者支持的文件系统中，spark把每条记录都转换为一行记录，然后写到file中。
saveAsSequenceFile(path)//只能用在key-value对上，然后生成SequenceFile写到本地或者hadoop文件系统
saveAsObjectFile(path)//把数据写到一个java序列化的文件中，用SparkContext.objectFile()装载
countByKey()// 返回的是key对应的个数的一个map,作用于一个RDD
foreach(func)//对dataset中的每个元素都使用func
```

缓存的操作

使用persist和cache方法将任意的RDD缓存到内存，磁盘文件系统中

缓存是容错的可以通过构建它的transformation自动重构

被缓存的RDD被使用的时，存取速度会被大大加速

persisit可以指定一个StorageLevel


__创建SparkContext步骤__

 + 导入Spark的类和隐式转换
 + 构建Spark应用程序的应用信息对象SparkConf
 + 利用SparkConf对象来初始化SparkContext
 + 创建RDD,并执行相应的Transformation和action并得到最后的结果


广播变量(Broadcast Variables)

广播变量缓存到各个节点的内存中，而不是每个Task

广播变量被创建后，能在集群中运行的任何函数调用

广播变量是只读的，不能在被广播后修改

对于大数据集的广播，Spark尝试使用高效的广播算法来降低通信成本

```scala
val broadcastVar = sc.broadcast(Array(1,2,3))
broadcastVar.value
```

累加器

累加器只支持加法操作，可以高效地并行，用于实现计数器和变量求和

Spark原生支持数值类型和标准可变集合的计数器，但用户可以添加新的类型

只有驱动程序才能获取累加器的值


核心模块---Schedule整体

Scheduler模块是Spark最核心的模块之一，充分体现了Spark与MapReduce的不同之处，体现了Spark DAG思想的精巧和设计的优雅。

Scheduler模块分为两大主要部分,DAGScheduler和TaskScheduler.

创建RDD，经过一系列Transformation,最后Action----\>Action会触发SparkContext的rubjob方法，交给DAGSchedule处理----\>DAGScheduler将DAG生成Stage-----\>将Stage交给TaskScheduler---\>本地或者集群的Executor上运行。

DAGScheduler把一个spark作业转换成Stage的DAG，根据RDD和Stage之间的关系，找出开销最小的调度方法，然后把stage以TaskSet的形式提交给TaskScheduler

SchedulableBuilder主要负责TaskSet的调度。核心接口:getSortedTaskSetQueue,该接口返回排序后的TaskSetManager队列，该接口供TaskSchedulerImpl调用。SchedulableBuilder维护的是一棵树，跟节点是rootpool,叶子节点是TaskSetManager对象。

TaskSet优先级排序有两种算法，一种是FIFO,另一种是Fair，所以SchedulableBuilder又两种实现:FIFO　SchedulableBuilder和FairSchedulableBuilder.

FIFO(默认模式)是先进先出型，根据TaskSet对应的stageID的顺序来调度。只有一个pool---rootpool和叶子节点---TaskSetManager.

Fair(模式)是公平调度型，根据所管理的pool/TaskSetManager中正在运行的任务的数量来判断优先级。所构建的调度池是两级的结构，即rootpool管理一组pool,子pool进一步管理属于该调度池的TaskSetManager.

TaskSetManager主要负责一个taskset中task的调度和跟踪

核心接口:resourceOffer,该接口根据输入的资源在taskset内部调度一个task,主要考虑因素是Locality,该接口供TaskSchedulerImpl调度。

 + 根据task的perferredLocations得到每个Task的Localitylevel
 + resourceOffe根据资源和maxLocality(最大宽松的本地化级别)调度task
 + 最终调度task的allowedLocality是该TaskSet允许的Locality(最大不超过输入的maxLocality),该TaskSet允许的Locality最初默认值是最严格本地化级别。如果lastLaunchTime(最近一次该taskset发布的task的时间)与当前时间差超时，会放宽locality的要求，选择低一优先级的locality.
 + 在allowedLocality范围内，优先调度更local的task,也就是最好在同个进程里，次好是同个node(机器)上，再次是同机架。在allowedLocality范围内，在该taskset没有找到task,那么返回None.(上一层调用会继续查询其他的taskset是否有满足指定localityLevel的task)

SchedulerBackend是trait,封装了多种backend,用于与底层资源调度系统交互(mesos/YARN),配合TaskScheduler实现具体任务所需要的资源分配。

核心接口:reviveOffers，与TaskSchedulerImpl交互完成task的Launch

SchedluerBackend值关心资源，不关心task.提交资源供TaskSchedulerImpl分配task

ReviveOffers的实现

 + 将空闲资源(freeCore,executor,host)以及workerOfferList形式组织。
 + 调用TaskSchedulerImpl的resourceOffers(),为workerOfferList空闲资源分配相应的task
 + 调用launchTasks，向executorActor发送LaunchTask消息。


```scala
val accum = sc.accumulator(0)
sc.parallelize(Array(1,2,3,4)).foreach(x => accum += x)
accum.value


```




```scala
/*SimpleApp.scala*/
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf

object SimpleApp{
	def main(args: Array[String]){
		val		logFile = "/home/user/spark/README.md"
		val		conf	= new SparkConf().setAppName("Scala Application")
		val		sc		= new SparkContext(conf)
		val		file	= sc.textFile(logFile)
		val		count	= file.flatMap(line=>line.split(" ")).map(word=>(word,1)).reduceByKey(_+_)
		count.collect()
		count.saveAsTextFile("/home/user/code/out/aa")
	}
}
```

__并行化集合__

```scala
//SparkContext的parallelize方法生成RDD
val		rdd = sc.parallelize(Array(1 to 10))
val		rdd = sc.parallelize(Array(1 to 10),5)// 指定了parition的数量
// 参数slice：启动的executro的数量来进行　切分多个slice,每个slice启动一个Task来处理。

```





**Spark-shell**

用Spark-shell来开发程序,在Spark-shell中`已经`创建一个名为sc的SparkContext对象和sqlcontext的SQL context对象,在4个CPU核上运行Spark-shell:

	cd `echo $SPARK_HOME`
	./bin/spark-shell --master local[4]
	#如果指定jar路径
	./bin/spark-shell --master local[4] --jars testcode.jar

其中，--master用来设置context将要连接并使用的`资源主节点`，master的值是Standalone模式的Spark集群地址，Meson或者YARN集群的URL,或者是一个local地址，使用--jars可以添加Jar的路径，使用`逗号`分割可以`添加多个包`，Spark-shell的本质是在后台调用了spark-submit脚本来启动`应用程序`.

__加载text文件__

Spark创建sc之后，就可以加载本地文件创建RDD,我们可以加载Spark自带的本地README.md文件进行测试，返回一个MapPartitionsRDD文件

	scala>val textFile = sc.textFile("file:///$SPARK_HOME/README.md")

需要说明的是，加载HDFS文件和本地文件都是使用textFile,区别是添加前缀(hdfs://和file://)进行标识，从本地读取文件直接返回MapPartitionsRDD,而从HDFS读取的文件先转换成HadoopRDD,然后`隐式转换成MapPartitionsRDD`。上面说的MapPartitionsRDD和HadoopRDD都是直接基于Spark的`弹性分布式数据集(RDD)`.

对于RDD，可以执行Transformation返回新的RDD，也可以执行Action得到返回结果

	// 获取RDD文件textFile的第一项
	scala>textFile.first()
	scala>textFile.count()

接下来通过Transformation操作，使用filter命令返回一个新的RDD，即抽取文件全部条目的一个子集，返回一个新的FilteredRDD

	//抽取含有"spark"的子集,可以链接多个Transformation和Action进行操作
	scala>textFile.filter(line => line.split("Spark")).count()

通过RDD进行组合，来实现找出文本中`每行最多的单词数`，`词频统计`等。

1.找出文本中每行最多单词数，基于RDD的Transformation和Action可以用作更复杂的运算

	scala>textFile.map(line => line.split(" ").size).reduce((a,b) => if (a > b) a else b)

split("")进行分词，并统计分词后的单词数，创建一个基于单词数的新的RDD，然后针对该RDD执行Reduce操作使用(a,b)=\> if (a,b) a else b

2.词频统计

从MapReduce开始，词频统计已经成为大数据处理最流行的入门程序，类似MapReduce，Spark也能很容易实现MapReduce

	//	结合flatMap,map和reduceBykey来计算文件中每个单词的词频
	scala>val wordCount = textFile.flatMap(line => line.split(" ")).map(word => (word,1)).reduceByKey((a,b) => a + b)
	// 使用collect聚合单词统计结果
	scala>wordCount.collect()

结合flatMap,map和reduceByKey来计算文件中每个单词的词频，并返回(string,int)类型的键值对ShuffleRDD(由于reduceByKey执行时需要进行Shuffle操作，返回的是一个Shuffle形式的RDD，ShuffleRDD)，最后使用collect聚合单词统计计算结果

__note__:如果想让Scala的函数文本更加的简洁，可以使用`占位符"_"`，占位符可以看做表达式里需要被`填入`的`空白`，这个`空白`在每次函数被调用时，由函数的`参数填入`.

当每个参数在函数文本中最多出现一次的 情况下，可以使用"_+_"扩展成两个参数的函数文本；在`多个下划线指代多个参数`，而不是`单个参数的重复使用`。第一个下划线代表第一个参数，第二个下划线代表第二个参数，依次类推。

下面通过占位符来对词频进行统计，进行优化

	scala>val wordCount = textFile.flatMap(_.split(" ")).map(_,1).reduceBeKey(_+_)

默认的Spark是`不进行排序的`，如果以排序的方式输出，需要进行key和value互换，然后采取sortByKey的方式，可以指定降序(false),升序(true),这样就完成的数据统计的排序

	scala>val wordCount = textFile.flatMap(_.split(" ")).map(_,1).reduceBeKey(_+_).map(x => (x._2,x._1)).sortByKey(false).map(x => (x._2,x._1))

上面的代码通过第一个x=>(x._2,x._1)实现key和value的互换，然后通过sortByKey(false)实现降序排列，通过第二个x => (x._2,x._1)再次互换，最后完成排序.

RDD 缓存 Spark也支持将数据集存放到一个集群的内存缓冲中，当数据被饭费访问时，如果在查询一个小而热的数据集，或运行一个像PageRank的迭代算法，是非常有用的，缓冲textFile变量

	scala>textFile.cache()
	scala>textFile.count()

通过cache缓存数据可以用于`非常大的数据集，支持跨越几十或几百个节点`。


__spark-submit提交方式__

master如下：
 + local	以单线程本地运行Spark(完全无并行)
 + local[K]	在本地以K个Worker线程运行Spark(这个数字设为你机器CPU核数的数目比较理想)
 + local[\*]	以与你机器上逻辑核数目相同的Worker线程运行Spark
 + spark://HOST:PORT	直接连接到给定的Spark独立模式集群上的Master，该端口必须设置好，可供使用的，一般是默认是7070
 + yarn-client	以Client模式连接到YARN集群，该集群的位置可以在HADOOP\_CONF\_DIR变量中找到
 + yarn-cluster	以Cluster模式连接到ARN集群，该集群的位置可以在HADOOP\_CONF\_DIR变量中找到

具体代码

```scala
#Local模式在4个CPU核上提交应用程序
$YOUR_SPAKR_HMOE/bin/spark-submit --classs "simpleApp" --master local[4] xxxx.jar argv
#Standalone模式提交应用程序
$YOUR_SPAKR_HMOE/bin/spark-submit --classs "simpleApp" --master spark://*.*.*.*:7077 --executor 2G --total-executor-core 10 xxxx.jar argv
#YARN模式提交应用程序
$YOUR_SPAKR_HMOE/bin/spark-submit --classs "simpleApp" --master yarn-cluster --executor 2G --total-executor-core 10 xxxx.jar argv
```

****
****
****
****
****
****
****
****
****
****
****
****
****
****
****
****
****
****

****


### docker-scala-spark

这里收集了一些关于scala，spark编程的docker.
dockoey/jupyter-scala
[dockerhub](https://hub.docker.com/r/dockoey/jupyter-scala/)
[github](https://github.com/dockoey/jupyter-scala)

all-spark-notebook
[dockerhub](https://hub.docker.com/r/jupyter/all-spark-notebook/)
[github](https://github.com/jupyter/docker-stacks)

### spark e-book

 + [Building a Recommendation Engine with Scala](https://www.safaribooksonline.com/library/view/building-a-recommendation/9781785282584/)
