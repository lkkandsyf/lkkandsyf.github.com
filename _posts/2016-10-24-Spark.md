---
layout: post
category : Big-data
tagline: "Supporting tagline"
tags : [ spark ]
---
{% include JB/setup %}

# Overview
{:.no_toc}

* dir
{:toc}

## Jupyter-spark

我们可以使用Apache提供的toree来使用。

[https://toree.incubator.apache.org/documentation/user/quick-start.html](https://toree.incubator.apache.org/documentation/user/quick-start.html)


启动了jupyter，可直接在http://localhost:4040/jobs/来监控所有的作业。

---

## Spark-command

**spark-shell**

Usage:spark-submit [options] <app jar| python file> [app options]

./spark-submit -h来查看参数列表

	参数名称								含义

	--master MATER_URL			可以是spark://host:port,mesos://host:port,yarn,yarn-cluster,yarn-client,local
	--deploy-mode DEPLOY_MODE	Driver程序运行的地方，client或者cluster
	--class CLASS_NAME			主类名称，含包名
	--name NAME					Application名称
	--jar JARS					Driver依赖的第三方jar包
	--py-files PY_FILES			用逗号隔开的放置在Python应用程序PYTHONPATH上的.zip.egg,.py文件列表
	--files FILES				用逗号隔开放置在每个executor工作目录的文件列表
	--properties-file FILE		设置应用程序属性的文件路径，默认是conf/spark-defaults.conf
	--dirver-memory MEM			Driver程序使用内存的大小
	--driver-java-options
	--driver-library-path		Driver程序的库路径
	--dirver-class-path			Driver程序的类路径
	--executor-memory MEM		executor内存大小，默认是1G
	--dirver-cores NUM			Driver程序使用的CPU个数，仅限Spark Alone模式
	--supervise					失败后是否重启Driver，仅限Spark Alone模式
	--total-executor-cores NUM	executor使用的总核数，仅限Spark Alone，Spark on Mesos模式
	--executor-cores NUM		每个executor使用的内核数，默认是1，仅限Spark on Yarn模式
	--queue QUEUE_NAME			提交应用程序给哪个YARN的队列，默认是default队列，仅限Spark on Yarn模式
	--num-executors NUM			启动executor数量，默认是2个，仅限Spark on Yarn模式
	--archive ARCHIVE			仅限Spark on Yarn模式







**pyspark**

**spark-submit**

****
****
****
****
****
****
****
****
****
****
****
****
****
****
****
****
****
****
****
****
****
