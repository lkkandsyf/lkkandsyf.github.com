---
layout: post
category : Hive
tagline: "Supporting tagline"
tags : []
---
在Spark中使用了数据库Hive,所以要对数据库也有一定的了解。

---
<!--more-->
---


## 基本知识

hive是基于Hadoop的一个数据仓库，可以将结构化的数据文件映射为一张数据库表，并提供完整的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。

hive是建立在Hadoop上的数据仓库基础框架，它提供了一些列的工具，可以用来进行数据提取转换加载，这是一种可以存储，查询，分析存储在Hadoop中的大规模数据的机制。Hive定义了简单的类SQL语句，称为HQL,它允许熟悉SQL的用户查询数据。同时，这个语言也允许熟悉MapReduce开发者的开发自定义的mapper和reduce来处理内建的mapper和reducer无法完成的复杂的分析工作。

使用hive的命令行接口，感觉`很像操作关系数据库`,但hive和关系数据库还有很大的不同。
　
 + hive和关系数据库存储文件的系统不同，hive使用的是Hadoop的HDFS(hadoop分布式文件系统)，关系数据库则是服务器本地的文件系统
 + hive使用的计算模型是mapreduce,而关系数据库则是自己设计的计算模型
 + 关系数据库都是为实时查询的业务进行设计的，而hive则是为海量数据挖掘设计的，实时性很差；实时性的区别导致hive的应用场景和关系数据库有很大的不同
 + hive很容易扩展自己的存储能力和计算能力，这个是继承Hadoop的，而关系数据库在这个方面比数据库差很多


hadoop和mapreduce是hive架构的根基。Hive架构包括如下组件：CLI（command line interface）、JDBC/ODBC、Thrift Server、WEB GUI、metastore和Driver(Complier、Optimizer和Executor)，这些组件我可以分为两大类：服务端组件和客户端组件。

hive三种运行方式

1. 内嵌模式

将元数据保存在本地内嵌的 Derby 数据库中，这是使用 Hive 最简单的方式。但是这种方式缺点也比较明显，因为一个内嵌的 Derby 数据库每次只能访问一个数据文件，这也就意味着它不支持多会话连接。

2. 本地模式

这种模式是将元数据保存在本地独立的数据库中（一般是 MySQL），这用就可以支持多会话和多用户连接了。

3. 远程模式

此模式应用于 Hive 客户端较多的情况。把 MySQL 数据库独立出来，将元数据保存在远端独立的 MySQL 服务中，避免了在每个客户端都安装 MySQL 服务从而造成冗余浪费的情况。


__首先讲讲服务端组件__：

Driver组件：该组件包括Complier、Optimizer和Executor，它的作用是将我们写的HiveQL（类SQL）语句进行解析、编译优化，生成执行计划，然后调用底层的mapreduce计算框架。

Metastore组件：元数据服务组件，这个组件存储hive的元数据，hive的元数据存储在关系数据库里，hive支持的关系数据库有derby、mysql。元数据对于hive十分重要，因此hive支持把metastore服务独立出来，安装到远程的服务器集群里，从而解耦hive服务和metastore服务，保证hive运行的健壮性，这个方面的知识，我会在后面的metastore小节里做详细的讲解。

Thrift服务：thrift是facebook开发的一个软件框架，它用来进行可扩展且跨语言的服务的开发，hive集成了该服务，能让不同的编程语言调用hive的接口。

__客户端组件__：

CLI：command line interface，命令行接口。

Thrift客户端：上面的架构图里没有写上Thrift客户端，但是hive架构的许多客户端接口是建立在thrift客户端之上，包括JDBC和ODBC接口。

WEBGUI：hive客户端提供了一种通过网页的方式访问hive所提供的服务。这个接口对应hive的hwi组件（hive web interface），使用前要启动hwi服务。

Hive的metastore组件是hive元数据集中存放地。Metastore组件包括两个部分：metastore服务和后台数据的存储。后台数据存储的介质就是关系数据库，例如hive默认的嵌入式磁盘数据库derby，还有mysql数据库。Metastore服务是建立在后台数据存储介质之上，并且可以和hive服务进行交互的服务组件，默认情况下，metastore服务和hive服务是安装在一起的，运行在同一个进程当中。我也可以把metastore服务从hive服务里剥离出来，metastore独立安装在一个集群里，hive远程调用metastore服务，这样我们可以把元数据这一层放到防火墙之后，客户端访问hive服务，就可以连接到元数据这一层，从而提供了更好的管理性和安全保障。使用远程的metastore服务，可以让metastore服务和hive服务运行在不同的进程里，这样也保证了hive的稳定性，提升了hive服务的效率。


很多问题就会清晰很多，具体如下：

关系数据库里，表的加载模式是在数据加载时候强制确定的（表的加载模式是指数据库存储数据的文件格式），如果加载数据时候发现加载的数据不符合模式，关系数据库则会拒绝加载数据，这个就叫“写时模式”，写时模式会在数据加载时候对数据模式进行检查校验的操作。Hive在加载数据时候和关系数据库不同，hive在加载数据时候不会对数据进行检查，也不会更改被加载的数据文件，而检查数据格式的操作是在查询操作时候执行，这种模式叫“读时模式”。在实际应用中，写时模式在加载数据时候会对列进行索引，对数据进行压缩，因此加载数据的速度很慢，但是当数据加载好了，我们去查询数据的时候，速度很快。但是当我们的数据是非结构化，存储模式也是未知时候，关系数据操作这种场景就麻烦多了，这时候hive就会发挥它的优势。

关系数据库一个重要的特点是可以对某一行或某些行的数据进行更新、删除操作，hive不支持对某个具体行的操作，hive对数据的操作只支持覆盖原数据和追加数据。Hive也不支持事务和索引。更新、事务和索引都是关系数据库的特征，这些hive都不支持，也不打算支持，原因是hive的设计是海量数据进行处理，全数据的扫描时常态，针对某些具体数据进行操作的效率是很差的，对于更新操作，hive是通过查询将原表的数据进行转化最后存储在新表里，这和传统数据库的更新操作有很大不同。
Hive也可以在hadoop做实时查询上做一份自己的贡献，那就是和hbase集成，hbase可以进行快速查询，但是hbase不支持类SQL的语句，那么此时hive可以给hbase提供sql语法解析的外壳，可以用类sql语句操作hbase数据库。

主要讲hive支持的数据模型，例如：数据库（database）、表（table）、分区（partition）和桶（bucket），还有hive文件存储的格式，还有hive支持的数据类型。第三篇文章就会讲到hiveQL的使用、以及结合mapreduce查询优化的技术和自定义函数，

## 外部表和内部表

### 区别
 + Hive创建内部表时(默认创建内部表),会将数据移动到数据仓库指向的路径;创建外部表(需要加关键字external),仅记录数据所在的路径,不对数据的位置做任何改变;
 + Hive删除表时,内部表的元数据和数据会被一起删除,`而外部表只删除元数据,不删除数据;`

### 场景
 + 外部表:比如某个公司的原始日志数据存放在一个目录中,多个部门对这些原始数据进行分析,那么创建外部表是明智选择,这样原始数据不会被删除;
 + 内部表:对原始数据或比较重要的中间数据进行建表存储;

### Case

## 数据导入

 + 原始文件是一个txt类似csv格式

 1.先上传文件到hdfs上
 2.创建带有分区的表和不带分区的临时表
 3.将数据导入临时表
```c
load data local inpath 'path' into table tablename;
select  * from tablename;
```
 4.从临时表导入分区表
```c
set  hive.exec.dynamic.partition=true;
set  hive.exec.dynamic.partition.mode=nonstrict;
set  hive.exec.max.dynamic.partitions.pernode=10000;
insert overwrite tablen tablename partiton(name) select * from tablename;
hdfs dfs -du -h /hive/warehouse/tablename
drop table tmptablename
rm -rf /path
```
 + 通过insert
 ```C
 #$HIVE_HOME/bin/hive
 create table students (student string, age int);
 insert into  table students values('Walker', 33);
 insert into  table students values('Sam', 33);
 insert into  table students values('Sally', 33);
 insert into  table students values('Sue', 72);
 insert into  table students values('George', 56);
 insert into  table students values('William', 64);
 insert into  table students values('Ellen', 24);
 insert into  table students values('Jose', 72);
 insert into  table students values('Li', 56);
 insert into  table students values('Chris', 64);
 insert into  table students values('Ellen', 24);
 insert into  table students values('Sue', 72);
 select * from students;
 ```
[office usage](https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients#HiveServer2Clients-BeelineExample)




## 基本sql语句

### basic
```c
CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name    -- (Note: TEMPORARY available in Hive 0.14.0 and later)
  [(col_name data_type [COMMENT col_comment], ... [constraint_specification])]
  [COMMENT table_comment]
  [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]
  [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS]
  [SKEWED BY (col_name, col_name, ...)                  -- (Note: Available in Hive 0.10.0 and later)]
     ON ((col_value, col_value, ...), (col_value, col_value, ...), ...)
     [STORED AS DIRECTORIES]
  [
   [ROW FORMAT row_format]
   [STORED AS file_format]
     | STORED BY 'storage.handler.class.name' [WITH SERDEPROPERTIES (...)]  -- (Note: Available in Hive 0.6.0 and later)
  ]
  [LOCATION hdfs_path]
  [TBLPROPERTIES (property_name=property_value, ...)]   -- (Note: Available in Hive 0.6.0 and later)
  [AS select_statement];   -- (Note: Available in Hive 0.5.0 and later; not supported for external tables)

CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name
  LIKE existing_table_or_view_name
  [LOCATION hdfs_path];

data_type
  : primitive_type
  | array_type
  | map_type
  | struct_type
  | union_type  -- (Note: Available in Hive 0.7.0 and later)

primitive_type
  : TINYINT
  | SMALLINT
  | INT
  | BIGINT
  | BOOLEAN
  | FLOAT
  | DOUBLE
  | DOUBLE PRECISION -- (Note: Available in Hive 2.2.0 and later)
  | STRING
  | BINARY      -- (Note: Available in Hive 0.8.0 and later)
  | TIMESTAMP   -- (Note: Available in Hive 0.8.0 and later)
  | DECIMAL     -- (Note: Available in Hive 0.11.0 and later)
  | DECIMAL(precision, scale)  -- (Note: Available in Hive 0.13.0 and later)
  | DATE        -- (Note: Available in Hive 0.12.0 and later)
  | VARCHAR     -- (Note: Available in Hive 0.12.0 and later)
  | CHAR        -- (Note: Available in Hive 0.13.0 and later)

array_type
  : ARRAY < data_type >

map_type
  : MAP < primitive_type, data_type >

struct_type
  : STRUCT < col_name : data_type [COMMENT col_comment], ...>

union_type
   : UNIONTYPE < data_type, data_type, ... >  -- (Note: Available in Hive 0.7.0 and later)

row_format
  : DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS TERMINATED BY char]
        [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char]
        [NULL DEFINED AS char]   -- (Note: Available in Hive 0.13 and later)
  | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]

file_format:
  : SEQUENCEFILE
  | TEXTFILE    -- (Default, depending on hive.default.fileformat configuration)
  | RCFILE      -- (Note: Available in Hive 0.6.0 and later)
  | ORC         -- (Note: Available in Hive 0.11.0 and later)
  | PARQUET     -- (Note: Available in Hive 0.13.0 and later)
  | AVRO        -- (Note: Available in Hive 0.14.0 and later)
  | JSONFILE    -- (Note: Available in Hive 4.0.0 and later)
  | INPUTFORMAT input_format_classname OUTPUTFORMAT output_format_classname

constraint_specification:
  : [, PRIMARY KEY (col_name, ...) DISABLE NOVALIDATE ]
    [, CONSTRAINT constraint_name FOREIGN KEY (col_name, ...) REFERENCES table_name(col_name, ...) DISABLE NOVALIDATE
```

 1.查看数据库的表 show databases/tables;

 2.切换数据库 use database\_name;

 3.查看表结构 desc table\_name;

 4.查看表详细属性 desc formatted table\_name;

 5.模糊搜索表 show tables like '\*name\*';

 6.查看分区信息 show partitions table\_name;

 7.查看分区数据 select * from table\_name where partition\_name = '20181110';

 8.分区表

+ 单分区：文件下目录下只有一级目录
```c
create table table_name (id bigint) partitioned by (date string);
create table table_name (id bigint) partitioned by (date string) store as parquet;
```

+ 多分区：表文件夹下出现多文件嵌套,当数据被加载至表中时，不会对数据进行任何转换
```c
create table table_name (id bigint) partitioned by (date string, hour string);
create table table_name (id bigint) partitioned by (date string, hour string) store as parquet;
```

 9.加载数据 overwrite 覆盖,追加不要overwrite
 LOAD DATA [LOCAL] INPATH ‘filepath’ [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 …)]


 10.删除表字段,增加表字段,修改表字段类型或名字

```C
ALTER TABLE 表名 CHANGE 旧字段 新字段 类型;
```

CREATE TABLE test (a STRING,b BIGINT);

如果需要删除b列

ALTER TABLE test REPLACE COLUMNS (a STRING);

如果需要增加c列,当前列的末尾

ALTER TABLE test add columns(c STRING);

如果你需要修改表字段的名字，类型不变

alter table test change c d STRING;

alter table test change c d STRING FIRST;把d字段放在第一列
alter table test change c d STRING after a;把d字段放在a的后面

如果你需要修改表字段的名字，类型也变化
alter table test change c d BIGINT FIRST;把d字段放在第一列
alter table test change c d BIGINT after a;把d字段放在a的后面


 删除分区 alter table table\_name drop partition(date='20181110',hour='11') ,分区的数据和元数据都会删除


 导出数据
 hdfs dfs -get /user/hive/warehouse/table\_name

 上传数据
 hdfs dfs -put table\_name/\* /user/hive/warehouse/table\_name

 删除数据
 hdfs dfs -rm -r /user/hive/warehouse/table\_name/\*

 hive不支持用insert语句一条一条的进行插入操作，也不支持update操作。数据是以load的方式加载到建立好的表中。数据一旦导入就不可以修改

[https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-Add/ReplaceColumns](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-Add/ReplaceColumns)

 hive 除了某些字段之外的剩余所有字段
 ```
 set hive.support.quoted.identifiers=None;
 select `(name|id|pwd)?+.+` from tableName;
 ```

## 动态分区

   动态分区：使用动态分区需要设置hive.exec.dynamic.partition参数值为true，默认值为false，在默认情况下，hive会假设主分区时静态分区，副分区使用动态分区；如果想都使用动态分区，需要设置set hive.exec.dynamic.partition.mode=nostrick，默认为strick

hive-site.xml

```c
<property>
	<name>hive.exec.dynamic.partition</name>
	<value>true</value>
</property>
<property>
	<name>hive.exec.dynamic.partition.mode</name>
	<value>nonstrict</value>
</property>
```

## metastore

1. 调节metastore的启动参数$HIVE\_HOME/bin/ext/，vim metastore.sh
```c
export HIVE_METASTORE_HADOOP_OPTS="-Xms4096m -Xmx4096m"
```
添加之后:
```c
THISSERVICE=metastore
export SERVICE_LIST="${SERVICE_LIST}${THISSERVICE} "

metastore() {
  echo "Starting Hive Metastore Server"
  CLASS=org.apache.hadoop.hive.metastore.HiveMetaStore
  if $cygwin; then
    HIVE_LIB=`cygpath -w "$HIVE_LIB"`
  fi
  JAR=${HIVE_LIB}/hive-service-*.jar

  # hadoop 20 or newer - skip the aux_jars option and hiveconf

  export HIVE_METASTORE_HADOOP_OPTS="-Xms4096m -Xmx4096m"
  export HADOOP_OPTS="$HIVE_METASTORE_HADOOP_OPTS $HADOOP_OPTS"
  exec $HADOOP jar $JAR $CLASS "$@"
}

metastore_help() {
  metastore -h
}

```
重启服务
```c
hive --service metastore
```


## 数据倾斜优化

 使map的输出数据更均匀的分布到reduce中去，是我们的最终目标

 在对多个表join连接操作时，将小表放在join的左边，大表放在Jion的右边，

 将在reducer中进行join操作时的小table放入内存，而大table通过stream方式读取

 列裁剪（Column pruning）：只有需要用到的列才进行输出

 谓词下推（Predicate pushdown）：尽早进行数据过滤(见图表 7中，下面为先处理的逻辑)，减少后续处理的数据量

 分区裁剪（Partition pruning）：只读取满足分区条件的文件

 合并小文件：文件数目过多，会给 HDFS 带来压力，并且会影响处理效率，可以通过合并 Map 和 Reduce 的结果文件来尽量消除这样的影响

 使用动态分区 数据格式orc,parquet,avro



## Reference

[https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients](https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients)

